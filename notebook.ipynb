{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-OG24Dz2zAR"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpT23YRI14P7"
      },
      "outputs": [],
      "source": [
        "!pip install torch numpy pettingzoo gymnasium pettingzoo[classic]\n",
        "!pip install imageio[ffmpeg]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ7VqtntzMX_"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "import logging\n",
        "import pettingzoo\n",
        "from pettingzoo.classic import tictactoe_v3, connect_four_v3, texas_holdem_no_limit_v6\n",
        "import imageio\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "from copy import deepcopy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor, wait\n",
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIa8ctK0E6y4"
      },
      "outputs": [],
      "source": [
        "logging.getLogger(\"pettingzoo\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"imageio_ffmpeg\").setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geP9CxJujCpF"
      },
      "outputs": [],
      "source": [
        "!rm -rf ./videos/*.mp4\n",
        "!rm -rf ./videos/*.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lw62jgohulI"
      },
      "outputs": [],
      "source": [
        "!mkdir ./videos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WHICH_TO_RUN = 'GMA' # GMA or EMS"
      ],
      "metadata": {
        "id": "z6pI3ZVcTUEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJrv5e6N8Zll"
      },
      "outputs": [],
      "source": [
        "config_GMA = {\n",
        "    'env_type': 'tictactoe_v3', # tictactoe_v3 e connect_four_v3, texas_holdem_no_limit_v6\n",
        "    'n_families': 4,\n",
        "    'family_size': 25,\n",
        "    'initial_std_dev': 0.1,\n",
        "    'min_std_dev': 0.001,\n",
        "    'std_dev_decay': 0.99, # TO REMOVE IT PUT IT TO 1\n",
        "    'family_n_elites': 1,\n",
        "    'n_generations': 1000,\n",
        "    'gamma': 0.99,\n",
        "    'neg_multiplier': 1.2,\n",
        "    'use_softmax': False,\n",
        "    'family_hof_size': 5,\n",
        "    'use_action_mask': True,\n",
        "    'plot_eval_freq': 1,\n",
        "    'plot_eval_times': 50,\n",
        "    'plot_eval_window': 20,\n",
        "    'plot_path': './reward_plot_episode',\n",
        "    'video_folder': \"./videos\",\n",
        "    'parallelization_type': 'no',\n",
        "    'network_type': 'ClassicNet'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDHbJjdSUbcF"
      },
      "outputs": [],
      "source": [
        "config_EMS = {\n",
        "    'env_type': 'tictactoe_v3', # tictactoe_v3 e connect_four_v3, texas_holdem_no_limit_v6\n",
        "    'n_families': 4,\n",
        "    'family_size': 25,\n",
        "    'initial_std_dev': 0.09,\n",
        "    'min_std_dev': 0.01,\n",
        "    'std_dev_decay': 0.995, # TO REMOVE IT PUT IT TO 1\n",
        "    'n_generations': 1000,\n",
        "    'gamma': 0.99,\n",
        "    'neg_multiplier': 1.2,\n",
        "    'normalize_gradient': False,\n",
        "    'family_hof_size': 5,\n",
        "    'learning_rate': 1,\n",
        "    'use_action_mask': True,\n",
        "    'plot_eval_freq': 1,\n",
        "    'plot_eval_times': 50,\n",
        "    'plot_eval_window': 20,\n",
        "    'use_softmax': False,\n",
        "    'plot_path': './reward_plot_episode',\n",
        "    'video_folder': \"./videos\",\n",
        "    'parallelization_type': 'no',\n",
        "    'network_type': 'ClassicNet'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRj8bs9UPI9H"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ilpPvHTPLue"
      },
      "outputs": [],
      "source": [
        "# cosine similarity between the vectors of weights of the models\n",
        "def cosine_similarity(model1, model2):\n",
        "\n",
        "    # Flatten the models' parameters into a single vector\n",
        "    model1_weights = torch.cat([p.view(-1) for p in model1.parameters()])\n",
        "    model2_weights = torch.cat([p.view(-1) for p in model2.parameters()])\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    cos_sim = F.cosine_similarity(model1_weights, model2_weights, dim=0)\n",
        "\n",
        "    return cos_sim.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMNexpEu9PIt"
      },
      "outputs": [],
      "source": [
        "# pool of workers for multi-process training\n",
        "class Pool:\n",
        "\n",
        "    def __init__(self, max_workers):\n",
        "        self.executor = ProcessPoolExecutor(max_workers=max_workers)\n",
        "        self.futures = []\n",
        "\n",
        "\n",
        "    def submit_task(self, func, *args, **kwargs):\n",
        "        future = self.executor.submit(func, *args, **kwargs)\n",
        "        self.futures.append(future)\n",
        "\n",
        "\n",
        "    def collect_results(self):\n",
        "\n",
        "        # Ensure all futures are completed\n",
        "        wait(self.futures)\n",
        "\n",
        "        # Collect results from the completed futures in the order they were submitted\n",
        "        results = [future.result() for future in self.futures]\n",
        "\n",
        "        # Reset futures list for future tasks\n",
        "        self.futures = []\n",
        "        return results\n",
        "\n",
        "\n",
        "    def shutdown(self):\n",
        "        self.executor.shutdown(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23O2Hhkzwh8W"
      },
      "source": [
        "# Plain Neural Network class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8txiFX8wgFL"
      },
      "outputs": [],
      "source": [
        "# a simple neural network adapted to the three explored environments\n",
        "class ClassicNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_shape = [3,3,2], n_actions = 9, bias = True):\n",
        "        super(ClassicNet, self).__init__()\n",
        "        if len(input_shape) == 1: # observation space is a vector\n",
        "            self.fc1 = nn.Linear(input_shape[0], 512, bias=bias)\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(2 * input_shape[0] * input_shape[1], 512, bias = bias)\n",
        "\n",
        "        self.fc2 = nn.Linear(512, 32, bias=bias)\n",
        "        self.fc3 = nn.Linear(32, n_actions, bias=True) #always false\n",
        "\n",
        "        # trying with 64 and 32 instead of 128 and 64\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))  # Using tanh activation function\n",
        "        x = torch.relu(self.fc2(x))  # Using tanh activation function\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c_S7yWzipYE"
      },
      "outputs": [],
      "source": [
        "# a simple deeper network\n",
        "class DeepNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_shape = [3,3,2], n_actions = 9, bias = True):\n",
        "        super(DeepNet, self).__init__()\n",
        "\n",
        "        if len(input_shape) == 1: #observation space is a vector\n",
        "          self.fc1 = nn.Linear(input_shape[0], 64, bias = bias)\n",
        "        else:\n",
        "          self.fc1 = nn.Linear(2 * input_shape[0]*input_shape[1], 64, bias = bias)\n",
        "\n",
        "        self.fc2 = nn.Linear(64, 32, bias = bias)\n",
        "        self.fc3 = nn.Linear(32, 32, bias = bias)\n",
        "        self.fc4 = nn.Linear(32, 16, bias = bias)\n",
        "        self.fc5 = nn.Linear(16, 16, bias = bias)\n",
        "        self.fc6 = nn.Linear(16, n_actions, bias = True)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = torch.relu(self.fc5(x))\n",
        "        x = self.fc6(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrpcvBc9vRAj"
      },
      "source": [
        "# Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWxV9wgqvXZD"
      },
      "outputs": [],
      "source": [
        "# agent using a neural network and working on classic environments of pettingzoo\n",
        "class NeuroAgentClassic(nn.Module):\n",
        "\n",
        "    def __init__(self, input_shape, n_actions, use_softmax, mode = 'training', network_type = ClassicNet):\n",
        "        super(NeuroAgentClassic, self).__init__()\n",
        "        assert mode in ['training', 'evaluating', 'deploying']\n",
        "        self.input_shape = input_shape\n",
        "        self.n_actions = n_actions\n",
        "        self.model = network_type(input_shape, n_actions)\n",
        "        self.use_softmax = use_softmax\n",
        "        self.mode = mode\n",
        "\n",
        "        # disable gradient for the model\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def save(self, filename):\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.state_dict(),\n",
        "            'input_shape': self.input_shape,\n",
        "            'n_actions': self.n_actions,\n",
        "            'use_softmax': self.use_softmax,\n",
        "            'mode': self.mode,\n",
        "        }\n",
        "        torch.save(checkpoint, filename)\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filename):\n",
        "        checkpoint = torch.load(filename, weights_only=False)\n",
        "        instance = cls(\n",
        "            input_shape=checkpoint['input_shape'],\n",
        "            n_actions=checkpoint['n_actions'],\n",
        "            use_softmax=checkpoint['use_softmax'],\n",
        "            mode=checkpoint['mode']\n",
        "        )\n",
        "        instance.load_state_dict(checkpoint['model_state_dict'])\n",
        "        return instance\n",
        "\n",
        "\n",
        "    def get_perturbable_layers(self):\n",
        "      return [m for m in self.model.modules() if isinstance(m, nn.Linear) or isinstance(m, type(nn.Conv2d))]\n",
        "\n",
        "\n",
        "    def get_perturbable_weights(self):\n",
        "        weights = []\n",
        "        for layer in self.get_perturbable_layers():\n",
        "            weights.append(layer.weight.data.cpu().numpy().flatten())\n",
        "            if hasattr(layer, 'bias') and layer.bias is not None:\n",
        "              weights.append(layer.bias.data.cpu().numpy().flatten())\n",
        "        return np.concatenate(weights)\n",
        "\n",
        "\n",
        "    def set_perturbable_weights(self, flat_weights):\n",
        "        idx = 0\n",
        "        for layer in self.get_perturbable_layers():\n",
        "            weight_size = layer.weight.numel()\n",
        "            layer.weight.data = torch.tensor(flat_weights[idx: idx + weight_size].reshape(layer.weight.shape))\n",
        "            idx += weight_size\n",
        "            if hasattr(layer, 'bias') and layer.bias is not None:\n",
        "              bias_size = layer.bias.numel()\n",
        "              layer.bias.data = torch.tensor(flat_weights[idx: idx + bias_size].reshape(layer.bias.shape))\n",
        "              idx += bias_size\n",
        "\n",
        "\n",
        "    # mutate the model's weights by adding a normally distribute noise\n",
        "    def mutate(self, std_dev):\n",
        "\n",
        "        # get weights to mutate\n",
        "        perturbable_weights = self.get_perturbable_weights()\n",
        "\n",
        "        # generate the noise\n",
        "        noise = np.random.normal(loc=0.0, scale=std_dev, size=perturbable_weights.shape).astype(np.float32)\n",
        "\n",
        "        weights = perturbable_weights + noise\n",
        "\n",
        "        # apply the noise\n",
        "        self.set_perturbable_weights(weights)\n",
        "\n",
        "        return noise\n",
        "\n",
        "\n",
        "    # choose best action or sample according to model's logits\n",
        "    def choose_action(self, inputs, action_mask):\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # get action values\n",
        "            logits = self.model(inputs).squeeze(0)\n",
        "            masked_logits = logits.clone()\n",
        "            masked_logits[action_mask == 0] = float('-inf')\n",
        "\n",
        "            # get probabilities\n",
        "            masked_probs = torch.nn.functional.softmax(masked_logits, dim=0)\n",
        "\n",
        "            # choose action\n",
        "            if self.mode == 'training' and self.use_softmax:\n",
        "                chosen_action = torch.multinomial(masked_probs, 1).item()\n",
        "            else:\n",
        "                # mode = evaluating, mode = deploying and mode = training with not softmax\n",
        "                chosen_action = torch.argmax(masked_probs).item()\n",
        "\n",
        "            return chosen_action, logits, masked_logits, masked_probs\n",
        "\n",
        "\n",
        "    # get number of parameters\n",
        "    def size(self):\n",
        "        num_params = sum(p.numel() for p in self.model.parameters())\n",
        "        print(\"Number of parameters:\", num_params)\n",
        "        param_size_mb = num_params * 4 / (1024 ** 2)\n",
        "        print(f\"Model size: {param_size_mb:.2f} MB\")\n",
        "        return num_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS1aQ_R-CS8w"
      },
      "outputs": [],
      "source": [
        "# simple agent choosing random actions working on classic environments of pettingzoo\n",
        "# (used as a baseline for evaluations)\n",
        "class DummyAgent(nn.Module):\n",
        "\n",
        "    def __init__(self, n_actions):\n",
        "        super(DummyAgent, self).__init__()\n",
        "        self.n_actions = n_actions\n",
        "        self.mode = 'evaluating'\n",
        "\n",
        "\n",
        "    def choose_action(self, inputs, action_mask):\n",
        "        valid_actions = np.where(action_mask == 1)[0]\n",
        "        logits = torch.zeros(self.n_actions)\n",
        "        masked_logits = torch.zeros(self.n_actions)\n",
        "        masked_logits[action_mask == 0] = float('-inf')\n",
        "        masked_probs = torch.ones(self.n_actions) / len(valid_actions)\n",
        "        masked_probs[action_mask == 0] = 0\n",
        "        return np.random.choice(valid_actions), logits, masked_logits, masked_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWEIfH5vwe-Y"
      },
      "source": [
        "# MultiTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owXYOahIwh8-"
      },
      "outputs": [],
      "source": [
        "# generic class implementing methods used by both approaches (enabled to work\n",
        "# with many 'families' and to train using multiple parallel processes)\n",
        "class MultiTrainer(ABC):\n",
        "\n",
        "    def __init__(self, env_type, n_families, family_size, n_generations, gamma, neg_multiplier,\n",
        "                 use_softmax, family_hof_size, initial_std_dev,\n",
        "                 min_std_dev, std_dev_decay, plot_eval_times, plot_eval_freq,\n",
        "                 plot_eval_window, use_action_mask, plot_path, video_folder, parallelization_type, network_type):\n",
        "\n",
        "        assert network_type in ['ClassicNet', 'DeepNet']\n",
        "        assert env_type in ['tictactoe_v3', 'connect_four_v3', 'texas_holdem_no_limit_v6']\n",
        "        assert parallelization_type in ['family', 'hof', 'no']\n",
        "\n",
        "        # training parameters\n",
        "        self.gamma = gamma\n",
        "        self.neg_multiplier = neg_multiplier\n",
        "        self.use_softmax = use_softmax\n",
        "        self.n_families = n_families\n",
        "        self.family_size = family_size\n",
        "        self.family_hof_size = family_hof_size\n",
        "        self.n_generations = n_generations\n",
        "        self.initial_std_dev = initial_std_dev\n",
        "        self.min_std_dev = min_std_dev\n",
        "        self.std_dev_decay = std_dev_decay\n",
        "        self.use_action_mask = use_action_mask\n",
        "        self.env_type = env_type\n",
        "        self.parallelization_type = parallelization_type\n",
        "\n",
        "        #choice of the network\n",
        "        if network_type == 'ClassicNet':\n",
        "          self.network_type = ClassicNet\n",
        "        elif network_type == 'DeepNet':\n",
        "          self.network_type = DeepNet\n",
        "\n",
        "        # parameters depending on the environment\n",
        "        self.render_mode = 'rgb_array' # rendering mode\n",
        "        if self.env_type == 'tictactoe_v3':\n",
        "            self.input_shape = [3,3,2]\n",
        "            self.n_actions = 9\n",
        "            self.players = ['player_1', 'player_2']\n",
        "        elif self.env_type == 'connect_four_v3':\n",
        "            self.input_shape = [6,7,2]\n",
        "            self.n_actions = 7\n",
        "            self.players = ['player_0', 'player_1']\n",
        "        elif self.env_type == 'texas_holdem_no_limit_v6':\n",
        "            self.input_shape = [54]\n",
        "            self.n_actions = 5\n",
        "            self.players = ['player_0', 'player_1']\n",
        "\n",
        "        # plot parameters\n",
        "        self.dummy = DummyAgent(self.n_actions)\n",
        "        self.plot_eval_window = plot_eval_window\n",
        "        self.plot_eval_times = plot_eval_times\n",
        "        self.plot_eval_freq = plot_eval_freq\n",
        "        self.plot_path = plot_path\n",
        "\n",
        "        # video parameters\n",
        "        self.video_folder = video_folder\n",
        "\n",
        "        # initialize variables\n",
        "        self.families_eval_rewards = [[] for i in range(self.n_families)]\n",
        "        self.families_train_rewards = [[] for i in range(self.n_families)]\n",
        "        self.families_mean_eval_rewards = [[] for i in range(self.n_families)]\n",
        "        self.step_count = 0\n",
        "        self.std_dev = self.initial_std_dev\n",
        "        self.winner = None\n",
        "        self.family_winners = [None for i in range(self.n_families)]\n",
        "\n",
        "\n",
        "    def transform_obs(self, obs):\n",
        "        if self.env_type == 'tictactoe_v3':\n",
        "            obs = obs.permute(2, 0, 1).unsqueeze(0)\n",
        "        elif self.env_type == 'connect_four_v3':\n",
        "            obs = obs.permute(2, 0, 1).unsqueeze(0)\n",
        "        elif self.env_type == 'texas_holdem_no_limit_v6':\n",
        "            obs = obs.unsqueeze(0)\n",
        "        return obs\n",
        "\n",
        "\n",
        "    def initialize_env(self):\n",
        "        if self.env_type == 'tictactoe_v3':\n",
        "          env = tictactoe_v3.env(render_mode=self.render_mode)\n",
        "        elif self.env_type == 'connect_four_v3':\n",
        "          env = connect_four_v3.env(render_mode=self.render_mode)\n",
        "        elif self.env_type == 'texas_holdem_no_limit_v6':\n",
        "          env = texas_holdem_no_limit_v6.env(render_mode=self.render_mode)\n",
        "        env.reset()\n",
        "        return env\n",
        "\n",
        "\n",
        "    # for parallel training at family level\n",
        "    def evaluate_family(self, family):\n",
        "\n",
        "        rewards = np.zeros(self.family_size)\n",
        "        for j in range(self.family_size):\n",
        "\n",
        "            for k in range(self.hof_size):\n",
        "                hof_index = -1-k\n",
        "                result = self.evaluate_agent(family[j], self.hof[hof_index], True)\n",
        "                rewards[j] += result\n",
        "\n",
        "        rewards /= self.hof_size\n",
        "        return rewards\n",
        "\n",
        "\n",
        "    # for parallel training at agent level\n",
        "    def evaluate_against_hof(self, agent):\n",
        "\n",
        "        reward = 0\n",
        "        for k in range(self.hof_size):\n",
        "            hof_index = -1-k\n",
        "            result = self.evaluate_agent(agent, self.hof[hof_index], True)\n",
        "            reward += result\n",
        "\n",
        "        reward /= self.hof_size\n",
        "        return reward\n",
        "\n",
        "\n",
        "    def schedule_parallel_training(self, families_population):\n",
        "\n",
        "      if self.parallelization_type == 'family':\n",
        "        for i in range(self.n_families):\n",
        "            WORKERS.submit_task(self.evaluate_family, families_population[i])\n",
        "        families_rewards = WORKERS.collect_results()\n",
        "\n",
        "      elif self.parallelization_type == 'hof':\n",
        "        for i in range(self.n_families):\n",
        "            for j in range(self.family_size):\n",
        "                WORKERS.submit_task(self.evaluate_against_hof, families_population[i][j])\n",
        "        families_rewards = WORKERS.collect_results()\n",
        "        families_rewards = np.reshape(families_rewards, (self.n_families, self.family_size))\n",
        "\n",
        "      elif self.parallelization_type == 'no':\n",
        "        families_rewards = []\n",
        "        for i in range(self.n_families):\n",
        "            for j in range(self.family_size):\n",
        "                reward = self.evaluate_against_hof(families_population[i][j])\n",
        "                families_rewards.append(reward)\n",
        "        families_rewards = np.reshape(families_rewards, (self.n_families, self.family_size))\n",
        "\n",
        "      return families_rewards\n",
        "\n",
        "\n",
        "    def play_game(self, agent1, agent2, save_video = False):\n",
        "\n",
        "        env = self.initialize_env()\n",
        "\n",
        "        if save_video:\n",
        "            path = self.video_folder + \"/epoch_\"+str(self.step_count)\n",
        "            frames = []\n",
        "            self.start_log(path)\n",
        "\n",
        "        total_rewards = [0, 0]\n",
        "        agents = [agent1, agent2]\n",
        "        steps = 0\n",
        "\n",
        "        for player in env.agent_iter():  # AEC mode!\n",
        "\n",
        "            observation, reward, termination, truncation, _ = env.last()\n",
        "            done = termination or truncation\n",
        "            steps += 1\n",
        "\n",
        "            player_id = self.players.index(player)\n",
        "            total_rewards[player_id] += reward\n",
        "\n",
        "            if done:\n",
        "                action = None\n",
        "\n",
        "            else:\n",
        "\n",
        "                mask = torch.tensor(observation[\"action_mask\"], dtype=torch.uint8)\n",
        "                obs = torch.tensor(observation['observation'], dtype=torch.float32)\n",
        "                obs = self.transform_obs(obs)\n",
        "\n",
        "                # an agent can do the wrong action in 'training'\n",
        "                if not self.use_action_mask and agents[player_id].mode == 'training':\n",
        "                    mask = torch.ones_like(mask)\n",
        "\n",
        "                action, logits, mlogits, probs = agents[player_id].choose_action(obs, mask)\n",
        "\n",
        "            env.step(action)\n",
        "\n",
        "            # save the rendered frame for the video and write log\n",
        "            if save_video:\n",
        "                frame = env.render()\n",
        "                frames.append(frame)\n",
        "                self.write_log(path, agents[player_id], player, action, logits, mlogits, probs)\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        if save_video:\n",
        "            self.compose_video(path, frames)\n",
        "\n",
        "        return total_rewards, steps-2\n",
        "\n",
        "\n",
        "    def custom_reward(self, reward, steps):\n",
        "        reward = reward * (self.gamma ** steps)\n",
        "        if reward < 0:\n",
        "            reward = reward * self.neg_multiplier\n",
        "        return reward\n",
        "\n",
        "\n",
        "    # evaluate the agent by playing against the evaluator\n",
        "    def evaluate_agent(self, agent, evaluator, use_custom_reward, times = 1):\n",
        "\n",
        "        total_reward = 0\n",
        "        agent.mode = 'training'\n",
        "        evaluator.mode = 'evaluating'\n",
        "\n",
        "        for i in range(times):\n",
        "\n",
        "            rewards, steps = self.play_game(evaluator, agent)\n",
        "            reward = self.custom_reward(rewards[1], steps) if use_custom_reward else rewards[1]\n",
        "            total_reward += reward\n",
        "\n",
        "            rewards, steps = self.play_game(agent, evaluator)\n",
        "            reward = self.custom_reward(rewards[0], steps) if use_custom_reward else rewards[0]\n",
        "            total_reward += reward\n",
        "\n",
        "        return total_reward / times\n",
        "\n",
        "\n",
        "    def record_play(self, agent1, agent2):\n",
        "        agent1.mode = 'deploying'\n",
        "        agent2.mode = 'deploying'\n",
        "        self.play_game(agent1, agent2, save_video = True)\n",
        "\n",
        "\n",
        "    # make the human play against the agent\n",
        "    def play_against(self, agent, start_first = True):\n",
        "\n",
        "        env = self.initialize_env()\n",
        "\n",
        "        agent.mode = 'deploying'\n",
        "\n",
        "        path = './play_against'\n",
        "        self.start_log(path)\n",
        "        frames = []\n",
        "        total_rewards = [0, 0]\n",
        "        steps = 0\n",
        "        user_position = 0 if start_first else 1\n",
        "\n",
        "        for player in env.agent_iter():  # AEC mode!\n",
        "\n",
        "            observation, reward, termination, truncation, _ = env.last()\n",
        "            done = termination or truncation\n",
        "            steps += 1\n",
        "\n",
        "            player_id = self.players.index(player)\n",
        "            total_rewards[player_id] += reward\n",
        "\n",
        "            # visualize current state\n",
        "            frame = env.render()\n",
        "            clear_output(wait=True)\n",
        "            fig, ax = plt.subplots(figsize=(6, 6))\n",
        "            ax.imshow(frame)\n",
        "            ax.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            if done:\n",
        "                action = None\n",
        "\n",
        "            else:\n",
        "\n",
        "                mask = torch.tensor(observation[\"action_mask\"], dtype=torch.uint8)\n",
        "                obs = torch.tensor(observation['observation'], dtype=torch.float32)\n",
        "                obs = self.transform_obs(obs)\n",
        "\n",
        "                frame = env.render()\n",
        "                frames.append(frame)\n",
        "\n",
        "                # human's turn\n",
        "                if user_position == player_id:\n",
        "\n",
        "                    # choose action\n",
        "                    print(\"Your Turn! Choose an action between 0 and\", self.n_actions - 1)\n",
        "                    print(\"Valid actions:\", [i for i, valid in enumerate(mask) if valid])\n",
        "                    while True:\n",
        "                        try:\n",
        "                            print(\"Enter your action: \")\n",
        "                            action = int(input())\n",
        "                            if action in [i for i, valid in enumerate(mask) if valid]:\n",
        "                                break\n",
        "                            else:\n",
        "                                print(\"Invalid action. Please choose a valid action.\")\n",
        "                        except ValueError:\n",
        "                            print(\"Invalid input. Please enter an integer.\")\n",
        "\n",
        "                # agent's turn\n",
        "                else:\n",
        "\n",
        "                    # choose action\n",
        "                    action, logits, mlogits, probs = agent.choose_action(obs, mask)\n",
        "                    self.write_log(path, agent, player, action, logits, mlogits, probs)\n",
        "\n",
        "            env.step(action)\n",
        "\n",
        "        # visualize current state\n",
        "        frame = env.render()\n",
        "        clear_output(wait=True)\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "        ax.imshow(frame)\n",
        "        ax.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        print(\"Game over!\")\n",
        "        print(\"You won!\" if total_rewards[user_position] > total_rewards[1-user_position] else \"You lose!\")\n",
        "\n",
        "        self.compose_video(path, frames)\n",
        "\n",
        "        return total_rewards, steps-2\n",
        "\n",
        "\n",
        "    # make the agent evaluate each configuration of a game\n",
        "    def evaluate_with_agent(self, agent):\n",
        "\n",
        "        env = self.initialize_env()\n",
        "\n",
        "        agent.mode = 'deploying'\n",
        "\n",
        "        path = './evaluate_with_agent'\n",
        "        self.start_log(path)\n",
        "        frames = []\n",
        "\n",
        "        for player in env.agent_iter():  # AEC mode!\n",
        "\n",
        "            observation, _, termination, truncation, _ = env.last()\n",
        "            done = termination or truncation\n",
        "\n",
        "            # visualize current state\n",
        "            frame = env.render()\n",
        "            clear_output(wait=True)\n",
        "            fig, ax = plt.subplots(figsize=(6, 6))\n",
        "            ax.imshow(frame)\n",
        "            ax.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            if done:\n",
        "                action = None\n",
        "\n",
        "            else:\n",
        "\n",
        "                mask = torch.tensor(observation[\"action_mask\"], dtype=torch.uint8)\n",
        "                obs = torch.tensor(observation['observation'], dtype=torch.float32)\n",
        "                obs = self.transform_obs(obs)\n",
        "\n",
        "                frame = env.render()\n",
        "                frames.append(frame)\n",
        "\n",
        "                # agent evaluation\n",
        "                action, logits, mlogits, probs = agent.choose_action(obs, mask)\n",
        "                self.write_log(path, agent, player, action, logits, mlogits, probs)\n",
        "\n",
        "                # human chooses action\n",
        "                print(\"Your Turn! Choose an action between 0 and\", self.n_actions - 1)\n",
        "                print(\"Valid actions:\", [i for i, valid in enumerate(mask) if valid])\n",
        "                while True:\n",
        "                    try:\n",
        "                        print(\"Enter your action: \")\n",
        "                        action = int(input())\n",
        "                        if action in [i for i, valid in enumerate(mask) if valid]:\n",
        "                            break\n",
        "                        else:\n",
        "                            print(\"Invalid action. Please choose a valid action.\")\n",
        "                    except ValueError:\n",
        "                        print(\"Invalid input. Please enter an integer.\")\n",
        "\n",
        "            env.step(action)\n",
        "\n",
        "        # visualize current state\n",
        "        frame = env.render()\n",
        "        clear_output(wait=True)\n",
        "        fig, ax = plt.subplots(figsize=(6, 6))\n",
        "        ax.imshow(frame)\n",
        "        ax.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        env.close()\n",
        "\n",
        "        print(\"Game over!\")\n",
        "\n",
        "        self.compose_video(path, frames)\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def train_step(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def initialize_train(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        # initialize variables\n",
        "        self.families_eval_rewards = [[] for i in range(self.n_families)]\n",
        "        self.families_train_rewards = [[] for i in range(self.n_families)]\n",
        "        self.families_mean_eval_rewards = [[] for i in range(self.n_families)]\n",
        "        self.step_count = 0\n",
        "        self.std_dev = self.initial_std_dev\n",
        "        self.initialize_train()\n",
        "\n",
        "        # training loop\n",
        "        for t in tqdm(range(self.n_generations)):\n",
        "\n",
        "            # train\n",
        "            self.train_step()\n",
        "\n",
        "            # evaluation\n",
        "            rewards  = []\n",
        "            for i in range(self.n_families):\n",
        "                reward = self.evaluate_agent(self.family_winners[i], self.dummy, False, self.plot_eval_times)\n",
        "                rewards.append(reward)\n",
        "            self.update_metrics(rewards)\n",
        "\n",
        "            if self.step_count % self.plot_eval_freq == 0 and self.step_count != 0:\n",
        "                self.plot_rewards()\n",
        "                self.plot_collected_rewards()\n",
        "\n",
        "            self.step_count+=1\n",
        "            self.std_dev = max(self.min_std_dev, self.std_dev * self.std_dev_decay)\n",
        "\n",
        "\n",
        "    def update_metrics(self, rewards):\n",
        "        for i in range(self.n_families):\n",
        "            self.families_eval_rewards[i].append(rewards[i])\n",
        "            n_elems = min(len(self.families_eval_rewards[i]), self.plot_eval_window)\n",
        "            reward_window = self.families_eval_rewards[i][-n_elems:]\n",
        "            self.families_mean_eval_rewards[i].append(np.mean(reward_window))\n",
        "\n",
        "\n",
        "    def plot_rewards(self):\n",
        "        # Ensure the colors are defined based on the number of families\n",
        "        colors = cm.tab10.colors if self.n_families <= 10 else cm.get_cmap('tab20', self.n_families).colors\n",
        "\n",
        "        # Create individual plots for each family\n",
        "        for i, (eval_rewards, train_rewards, mean_eval_rewards) in enumerate(zip(self.families_eval_rewards, self.families_train_rewards, self.families_mean_eval_rewards)):\n",
        "            plt.figure()\n",
        "            plt.clf()\n",
        "            plt.title(f'Family {i+1} Rewards - {self.step_count} steps')\n",
        "            plt.xlabel('Episode')\n",
        "            plt.ylabel('Reward')\n",
        "\n",
        "            color = colors[i % len(colors)]\n",
        "            plt.plot(eval_rewards, label=f'Episode Reward (current)', color=color, linestyle='-')\n",
        "            plt.plot(train_rewards, label=f\" Train HOF Episodes Reward (current)\", color=color, linestyle=':')\n",
        "            plt.plot(mean_eval_rewards, label=f\"{self.plot_eval_window}-Episodes Reward (window)\", color=color, linestyle='--')\n",
        "\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "\n",
        "            # Save the individual plot for this family\n",
        "            plt.savefig(f\"{self.plot_path}_family_{i+1}.png\", bbox_inches=\"tight\")\n",
        "            plt.close()  # Close the figure to free memory\n",
        "\n",
        "\n",
        "        # Create a sigle plot with individual subplots for each family\n",
        "        num_rows = 2\n",
        "        num_cols = math.ceil(self.n_families / num_rows)\n",
        "\n",
        "        fig, axes = plt.subplots(num_rows, num_cols, figsize=(7*num_cols, 10))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, (eval_rewards, train_rewards, mean_eval_rewards) in enumerate(zip(self.families_eval_rewards, self.families_train_rewards, self.families_mean_eval_rewards)):\n",
        "            ax = axes[i]\n",
        "            ax.set_title(f'Family {i+1} Rewards - {self.step_count} steps')\n",
        "            ax.set_xlabel('Episode')\n",
        "            ax.set_ylabel('Reward')\n",
        "\n",
        "            color = colors[i % len(colors)]\n",
        "            ax.plot(eval_rewards, label=f'Episode Reward (current)', color=color, linestyle='-')\n",
        "            ax.plot(train_rewards, label=f\"Train HOF Episodes Reward (current)\", color=color, linestyle=':')\n",
        "            ax.plot(mean_eval_rewards, label=f\"{self.plot_eval_window}-Episodes Reward (window)\", color=color, linestyle='--')\n",
        "\n",
        "            ax.legend()\n",
        "            ax.grid()\n",
        "\n",
        "        for j in range(i + 1, len(axes)):\n",
        "            fig.delaxes(axes[j])\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(f\"{self.plot_path}_subplots.png\", bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "        # Create a combined plot for all average rewards\n",
        "        plt.figure()\n",
        "        plt.clf()\n",
        "        plt.title(f'All Families Average Rewards (window {self.plot_eval_window}) - {self.step_count} steps')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Reward')\n",
        "\n",
        "        for i, mean_eval_rewards in enumerate(self.families_mean_eval_rewards):\n",
        "            color = colors[i % len(colors)]\n",
        "            plt.plot(mean_eval_rewards, label=f\"Family {i+1}\", color=color, linestyle='--')\n",
        "\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        # Save the combined plot\n",
        "        plt.savefig(f\"{self.plot_path}_all_families.png\", bbox_inches=\"tight\")\n",
        "        plt.close()  # Close the figure to free memory\n",
        "\n",
        "\n",
        "    def plot_collected_rewards(self):\n",
        "        plt.figure(1)\n",
        "        plt.clf()\n",
        "        plt.title(f'{self.step_count} steps - Average Rewards')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Reward')\n",
        "\n",
        "        avg_eval_rewards = np.mean(np.array(self.families_eval_rewards), axis=0)\n",
        "        avg_mean_eval_rewards = np.mean(np.array(self.families_mean_eval_rewards), axis=0)\n",
        "\n",
        "        max_eval_rewards = np.max(np.array(self.families_eval_rewards), axis=0)\n",
        "        max_mean_eval_rewards = np.max(np.array(self.families_mean_eval_rewards), axis=0)\n",
        "\n",
        "        plt.plot(avg_eval_rewards, label='Average Episode Reward', color='blue', linestyle='-')\n",
        "        plt.plot(avg_mean_eval_rewards, color='blue', linestyle='--')\n",
        "        plt.plot(max_eval_rewards, label='Max Episode Reward', color='red', linestyle='-')\n",
        "        plt.plot(max_mean_eval_rewards, color='red', linestyle='--')\n",
        "\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "\n",
        "        avg_plot_path = self.plot_path + '_collected.png'\n",
        "        plt.savefig(avg_plot_path, bbox_inches=\"tight\")\n",
        "        plt.close()  # Close the figure to free memory\n",
        "\n",
        "\n",
        "    def start_log(self, path):\n",
        "        path = path + '.txt'\n",
        "        with open(path, 'w') as file:\n",
        "            file.write('Starting game:\\n\\n')\n",
        "\n",
        "\n",
        "    def write_log(self, path, agent, player, action, logits, mlogits, probs):\n",
        "\n",
        "        path = path + '.txt'\n",
        "        last_layer = list(agent.model.children())[-1]\n",
        "        has_bias = hasattr(last_layer, 'bias') and last_layer.bias is not None\n",
        "\n",
        "        with open(path, 'a') as file:\n",
        "            file.write(\"-\"*91 + \"\\n\")\n",
        "            file.write(f\"{'Agent':<10} {player}\\n\")\n",
        "            file.write(f\"{'Action':<10} {action}\\n\")\n",
        "            file.write(\"-\"*91 + \"\\n\")\n",
        "            file.write(f\"{' ':<10} {' '.join(f'{val:>8}' for val in range(self.n_actions))}\\n\")\n",
        "            file.write(f\"{'MLogits':<10} {' '.join(f'{val:>8.4f}' for val in mlogits.flatten())}\\n\")\n",
        "            file.write(f\"{'Logits':<10} {' '.join(f'{val:>8.4f}' for val in logits.flatten())}\\n\")\n",
        "            if has_bias:\n",
        "                file.write(f\"{'Bias':<10} {' '.join(f'{val:>8.4f}' for val in last_layer.bias.flatten())}\\n\")\n",
        "                differences = logits.flatten() - last_layer.bias.flatten()\n",
        "                file.write(f\"{'Diff':<10} {' '.join(f'{val:>8.4f}' for val in differences)}\\n\")\n",
        "            file.write(f\"{'Probs':<10} {' '.join(f'{val:>8.4f}' for val in probs.flatten())}\\n\")\n",
        "            file.write(\"-\"*91 + \"\\n\\n\\n\")\n",
        "\n",
        "\n",
        "    def compose_video(self, path, frames):\n",
        "        path = path + '.mp4'\n",
        "        with imageio.get_writer(path, fps=15) as writer:\n",
        "            for frame in frames:\n",
        "                for i in range(15):\n",
        "                    writer.append_data(np.array(frame))\n",
        "\n",
        "\n",
        "    def save_winner(self, filename):\n",
        "        self.winner.save(filename)\n",
        "        return True\n",
        "\n",
        "\n",
        "    def save_winners(self, filename):\n",
        "        for i in range(self.n_families):\n",
        "            name = filename.replace('.pt', f'{i}.pt')\n",
        "            self.family_winners[i].save(name)\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9ML8tZY1K1q"
      },
      "source": [
        "# Genetic MultiTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HgHeVZL1Mtc"
      },
      "outputs": [],
      "source": [
        "# specific trainer for genetic algorithm approach\n",
        "class GeneticMultiTrainer(MultiTrainer):\n",
        "\n",
        "    def __init__(self, env_type, n_families, family_size, n_generations, gamma, neg_multiplier,\n",
        "                 use_softmax, family_hof_size, family_n_elites,\n",
        "                 initial_std_dev, min_std_dev, std_dev_decay, plot_eval_times,\n",
        "                 plot_eval_freq, plot_eval_window, use_action_mask, plot_path, video_folder, parallelization_type, network_type):\n",
        "\n",
        "        super().__init__(env_type, n_families, family_size, n_generations, gamma, neg_multiplier,\n",
        "                         use_softmax, family_hof_size, initial_std_dev,\n",
        "                         min_std_dev, std_dev_decay, plot_eval_times, plot_eval_freq,\n",
        "                         plot_eval_window, use_action_mask, plot_path, video_folder, parallelization_type, network_type)\n",
        "\n",
        "        assert family_n_elites < family_size\n",
        "\n",
        "\n",
        "        self.family_n_elites = family_n_elites\n",
        "        self.hof_size = self.family_hof_size * self.n_families\n",
        "\n",
        "\n",
        "    def initialize_train(self):\n",
        "\n",
        "        # randomly initialized elites (they are initialized with training but each time they are used we set the mode)\n",
        "        self.elites = []\n",
        "        for i in range(self.n_families):\n",
        "            family_elites = [NeuroAgentClassic(self.input_shape, self.n_actions, self.use_softmax, network_type = self.network_type) for _ in range(self.family_n_elites)]\n",
        "            self.elites.append(family_elites)\n",
        "\n",
        "        # hall of fame initially filled with the elites\n",
        "        self.hof = deque([], maxlen = self.hof_size)\n",
        "        j = 0\n",
        "        while(len(self.hof) < self.hof_size):\n",
        "            for i in range(self.n_families):\n",
        "                model = deepcopy(self.elites[i][j % self.family_n_elites])\n",
        "                self.hof.append(model)\n",
        "            j += 1\n",
        "\n",
        "        # winner is the last from the hall of fame\n",
        "        self.winner = self.hof[-1]\n",
        "        self.family_winners = []\n",
        "        for i in range(self.n_families):\n",
        "            hof_index = -1-i\n",
        "            self.family_winners.append(self.hof[hof_index])\n",
        "\n",
        "\n",
        "    def train_step(self):\n",
        "\n",
        "        # the first members of the population are the elites\n",
        "        families_population = []\n",
        "        for i in range(self.n_families):\n",
        "            family_population = self.elites[i][:self.family_n_elites]\n",
        "            families_population.append(family_population)\n",
        "\n",
        "        # the others are mutations of the elites\n",
        "        for i in range(self.n_families):\n",
        "            for j in range(self.family_size - self.family_n_elites):\n",
        "                father_id = j % self.family_n_elites\n",
        "                agent = deepcopy(self.elites[i][father_id])\n",
        "                agent.mutate(self.std_dev)\n",
        "                families_population[i].append(agent)\n",
        "\n",
        "        # compute score for each member of the population\n",
        "        families_rewards = self.schedule_parallel_training(families_population=families_population)\n",
        "\n",
        "        # update winner and hall of fame\n",
        "        best_rewards = np.zeros(self.n_families)\n",
        "        for i in range(self.n_families):\n",
        "            best_id = np.argmax(families_rewards[i])\n",
        "            best_rewards[i] = np.max(families_rewards[i])\n",
        "            self.family_winners[i] = deepcopy(families_population[i][best_id])\n",
        "            self.families_train_rewards[i].append(best_rewards[i])\n",
        "            self.hof.append(self.family_winners[i])\n",
        "\n",
        "        best_family = np.argmax(best_rewards)\n",
        "        self.winner = self.family_winners[best_family]\n",
        "        self.record_play(self.winner, self.winner)\n",
        "\n",
        "        # update the elites\n",
        "        for i in range(self.n_families):\n",
        "            new_elite_ids = np.argsort(families_rewards[i])[-self.family_n_elites:]\n",
        "            for j in range(self.family_n_elites):\n",
        "                id = new_elite_ids[j]\n",
        "                self.elites[i][j] = families_population[i][id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcKRlACc979N"
      },
      "outputs": [],
      "source": [
        "if WHICH_TO_RUN == 'GMA':\n",
        "    trainer = GeneticMultiTrainer(**config_GMA)\n",
        "    if config_GMA['parallelization_type'] != 'no':\n",
        "        WORKERS = Pool(max_workers = os.cpu_count())\n",
        "    trainer.train()\n",
        "    trainer.save_winners(\"./winner.pt\")\n",
        "    trainer.save_winner(\"./best.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFJJQTpHD7ID"
      },
      "source": [
        "# Evolutionary MultiTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJGjZuaKD_UH"
      },
      "outputs": [],
      "source": [
        "# specific trainer for evolutionary strategy approach\n",
        "class EvolutionMultiTrainer(MultiTrainer):\n",
        "\n",
        "    def __init__(self, env_type, n_families, family_size, n_generations, gamma, neg_multiplier, normalize_gradient,\n",
        "                 use_softmax, family_hof_size, initial_std_dev, min_std_dev,\n",
        "                 std_dev_decay, learning_rate, plot_eval_times, plot_eval_freq,\n",
        "                 plot_eval_window, use_action_mask, plot_path, video_folder, parallelization_type, network_type):\n",
        "\n",
        "        super().__init__(env_type, n_families, family_size, n_generations, gamma, neg_multiplier,\n",
        "                         use_softmax, family_hof_size, initial_std_dev,\n",
        "                         min_std_dev, std_dev_decay, plot_eval_times, plot_eval_freq,\n",
        "                         plot_eval_window, use_action_mask, plot_path, video_folder, parallelization_type, network_type)\n",
        "\n",
        "        self.learning_rate = learning_rate\n",
        "        self.hof_size = self.n_families * self.family_hof_size\n",
        "        self.normalize_gradient = normalize_gradient\n",
        "\n",
        "\n",
        "    def initialize_train(self):\n",
        "\n",
        "        # randomly initialized starting agent (initialized with training but each time they are used we set the mode)\n",
        "        self.family_winners = []\n",
        "        for i in range(self.n_families):\n",
        "            self.family_winners.append(NeuroAgentClassic(self.input_shape, self.n_actions, self.use_softmax, network_type = self.network_type))\n",
        "\n",
        "        self.winner = self.family_winners[0]\n",
        "\n",
        "        # hall of fame filled with copies of the starting agent\n",
        "        self.hof = deque([], maxlen = self.hof_size)\n",
        "        while(len(self.hof) < self.hof_size):\n",
        "            for i in range(self.n_families):\n",
        "                model = deepcopy(self.family_winners[i])\n",
        "                self.hof.append(model)\n",
        "\n",
        "\n",
        "    def train_step(self):\n",
        "\n",
        "        families_population = []\n",
        "        families_noises = []\n",
        "\n",
        "        for i in range(self.n_families):\n",
        "\n",
        "            family_population = []\n",
        "            family_noises = []\n",
        "\n",
        "            # populate with mutations of the current agent\n",
        "            for j in range(self.family_size):\n",
        "                agent = deepcopy(self.family_winners[i])\n",
        "                noise = agent.mutate(self.std_dev)\n",
        "                family_population.append(agent)\n",
        "                family_noises.append(noise)\n",
        "\n",
        "            families_population.append(family_population)\n",
        "            families_noises.append(family_noises)\n",
        "\n",
        "\n",
        "        # compute score for each member of the population\n",
        "        families_rewards = self.schedule_parallel_training(families_population=families_population)\n",
        "\n",
        "        mean_family_rewards = np.array([np.mean(i) for i in families_rewards])\n",
        "        for i in range(self.n_families):\n",
        "            self.families_train_rewards[i].append(mean_family_rewards[i])\n",
        "            if self.normalize_gradient: # TRY TO NORMALIZE\n",
        "              families_rewards[i] -= mean_family_rewards[i]\n",
        "\n",
        "        # compute gradients\n",
        "        gradients = []\n",
        "        for i in range(self.n_families):\n",
        "            gradient = np.zeros_like(families_noises[i][0])\n",
        "            for j in range(self.family_size):\n",
        "                # if not self.normalize_gradient or families_rewards[i][j] > 0:\n",
        "                gradient += families_noises[i][j] * families_rewards[i][j]\n",
        "            gradient *= self.learning_rate / (self.family_size * self.std_dev)\n",
        "            gradients.append(gradient)\n",
        "\n",
        "        # update weights\n",
        "        for i in range(self.n_families):\n",
        "            self.family_winners[i] = deepcopy(self.family_winners[i])\n",
        "            new_weights = self.family_winners[i].get_perturbable_weights() + gradients[i]\n",
        "            self.family_winners[i].set_perturbable_weights(new_weights)\n",
        "            self.hof.append(self.family_winners[i])\n",
        "\n",
        "        # ESTIMATE THE BEST FAMILY\n",
        "        best_family = np.argmax(mean_family_rewards)\n",
        "        self.winner = self.family_winners[best_family]\n",
        "        self.record_play(self.winner, self.winner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HUoc_ZTBVMRt"
      },
      "outputs": [],
      "source": [
        "if WHICH_TO_RUN == 'EMS':\n",
        "    trainer = EvolutionMultiTrainer(**config_EMS)\n",
        "    if config_EMS['parallelization_type'] != 'no':\n",
        "        WORKERS = Pool(max_workers = os.cpu_count())\n",
        "    trainer.train()\n",
        "    trainer.save_winners(\"./winner.pt\")\n",
        "    trainer.save_winner(\"./best.pt\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uRj8bs9UPI9H",
        "23O2Hhkzwh8W",
        "SrpcvBc9vRAj",
        "WWEIfH5vwe-Y"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}